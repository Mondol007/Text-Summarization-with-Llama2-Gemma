{
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8008863,
          "sourceType": "datasetVersion",
          "datasetId": 4717182
        },
        {
          "sourceId": 8085758,
          "sourceType": "datasetVersion",
          "datasetId": 4773033
        },
        {
          "sourceId": 8086168,
          "sourceType": "datasetVersion",
          "datasetId": 4773314
        },
        {
          "sourceId": 8094382,
          "sourceType": "datasetVersion",
          "datasetId": 4779050
        },
        {
          "sourceId": 8095026,
          "sourceType": "datasetVersion",
          "datasetId": 4779519
        },
        {
          "sourceId": 11261,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 8332,
          "modelId": 3301
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4190.684325,
      "end_time": "2023-12-17T22:21:21.061832",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-12-17T21:11:30.377507",
      "version": "2.4.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117"
      ],
      "metadata": {
        "papermill": {
          "duration": 12.837112,
          "end_time": "2023-12-17T21:11:46.656292",
          "exception": false,
          "start_time": "2023-12-17T21:11:33.81918",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-05-04T09:42:35.346415Z",
          "iopub.execute_input": "2024-05-04T09:42:35.347503Z",
          "iopub.status.idle": "2024-05-04T09:42:48.803657Z",
          "shell.execute_reply.started": "2024-05-04T09:42:35.347462Z",
          "shell.execute_reply": "2024-05-04T09:42:48.802635Z"
        },
        "trusted": true,
        "id": "_-olW-2wG2QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers==\"4.38.2\"\n",
        "!pip install -q accelerate\n",
        "!pip install -q -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install -q -U datasets"
      ],
      "metadata": {
        "papermill": {
          "duration": 25.835818,
          "end_time": "2023-12-17T21:12:12.500193",
          "exception": false,
          "start_time": "2023-12-17T21:11:46.664375",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-05-04T09:42:48.806142Z",
          "iopub.execute_input": "2024-05-04T09:42:48.807082Z",
          "iopub.status.idle": "2024-05-04T09:43:39.791572Z",
          "shell.execute_reply.started": "2024-05-04T09:42:48.807039Z",
          "shell.execute_reply": "2024-05-04T09:43:39.790252Z"
        },
        "trusted": true,
        "id": "4iwsIKDqG2QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/trl\n",
        "!pip install -q -U git+https://github.com/huggingface/peft"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:43:39.793182Z",
          "iopub.execute_input": "2024-05-04T09:43:39.793522Z",
          "iopub.status.idle": "2024-05-04T09:44:35.946958Z",
          "shell.execute_reply.started": "2024-05-04T09:43:39.793492Z",
          "shell.execute_reply": "2024-05-04T09:44:35.945860Z"
        },
        "trusted": true,
        "id": "Q2wOj78lG2QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "papermill": {
          "duration": 0.015679,
          "end_time": "2023-12-17T21:12:12.539544",
          "exception": false,
          "start_time": "2023-12-17T21:12:12.523865",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-05-04T09:44:35.949375Z",
          "iopub.execute_input": "2024-05-04T09:44:35.949704Z",
          "iopub.status.idle": "2024-05-04T09:44:35.955016Z",
          "shell.execute_reply.started": "2024-05-04T09:44:35.949675Z",
          "shell.execute_reply": "2024-05-04T09:44:35.953946Z"
        },
        "trusted": true,
        "id": "yE4yPeZnG2QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.01423,
          "end_time": "2023-12-17T21:12:12.576944",
          "exception": false,
          "start_time": "2023-12-17T21:12:12.562714",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-05-04T09:44:35.956820Z",
          "iopub.execute_input": "2024-05-04T09:44:35.957168Z",
          "iopub.status.idle": "2024-05-04T09:44:35.972834Z",
          "shell.execute_reply.started": "2024-05-04T09:44:35.957142Z",
          "shell.execute_reply": "2024-05-04T09:44:35.971942Z"
        },
        "trusted": true,
        "id": "4LJQ_GCuG2QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U datasets==2.17.0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:44:35.974062Z",
          "iopub.execute_input": "2024-05-04T09:44:35.974365Z",
          "iopub.status.idle": "2024-05-04T09:44:49.371465Z",
          "shell.execute_reply.started": "2024-05-04T09:44:35.974338Z",
          "shell.execute_reply": "2024-05-04T09:44:49.370291Z"
        },
        "trusted": true,
        "id": "jHNFdAzHG2QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import transformers\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          TrainingArguments,\n",
        "                          pipeline,\n",
        "                          logging)\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftConfig\n",
        "import bitsandbytes as bnb\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "papermill": {
          "duration": 19.450408,
          "end_time": "2023-12-17T21:12:32.05101",
          "exception": false,
          "start_time": "2023-12-17T21:12:12.600602",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-05-04T09:44:49.372914Z",
          "iopub.execute_input": "2024-05-04T09:44:49.373225Z",
          "iopub.status.idle": "2024-05-04T09:44:58.600366Z",
          "shell.execute_reply.started": "2024-05-04T09:44:49.373196Z",
          "shell.execute_reply": "2024-05-04T09:44:58.599267Z"
        },
        "trusted": true,
        "id": "aJN7kt_jG2QJ",
        "outputId": "7f6e591e-ac7b-48b2-c27c-ea0e00f543b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-05-04 09:44:53.876311: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-04 09:44:53.876388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-04 09:44:53.878001: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"/kaggle/input/gemma/transformers/7b-it/1\"\n",
        "\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "EOS_TOKEN = tokenizer.eos_token"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:44:58.601668Z",
          "iopub.execute_input": "2024-05-04T09:44:58.602497Z",
          "iopub.status.idle": "2024-05-04T09:45:15.269392Z",
          "shell.execute_reply.started": "2024-05-04T09:44:58.602458Z",
          "shell.execute_reply": "2024-05-04T09:45:15.268388Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "6fc8c600ebec407eaf458de79fc9b8a1"
          ]
        },
        "id": "QPDLJqMhG2QJ",
        "outputId": "abca85b0-5ec8-45dc-90a6-afd82c202aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fc8c600ebec407eaf458de79fc9b8a1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:15.270942Z",
          "iopub.execute_input": "2024-05-04T09:45:15.271869Z",
          "iopub.status.idle": "2024-05-04T09:45:15.280725Z",
          "shell.execute_reply.started": "2024-05-04T09:45:15.271830Z",
          "shell.execute_reply": "2024-05-04T09:45:15.279556Z"
        },
        "trusted": true,
        "id": "K2OYafhAG2QK",
        "outputId": "53d8b09e-40b7-4a9d-ddf6-26652ce2bfba"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "GemmaConfig {\n  \"_name_or_path\": \"/kaggle/input/gemma/transformers/7b-it/1\",\n  \"architectures\": [\n    \"GemmaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"head_dim\": 256,\n  \"hidden_act\": \"gelu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 24576,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"gemma\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 16,\n  \"pad_token_id\": 0,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"float16\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": false,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.38.2\",\n  \"use_cache\": false,\n  \"vocab_size\": 256000\n}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:15.283829Z",
          "iopub.execute_input": "2024-05-04T09:45:15.284130Z",
          "iopub.status.idle": "2024-05-04T09:45:15.295453Z",
          "shell.execute_reply.started": "2024-05-04T09:45:15.284104Z",
          "shell.execute_reply": "2024-05-04T09:45:15.294453Z"
        },
        "trusted": true,
        "id": "UqjTS4CIG2QK",
        "outputId": "457b7a0b-f4cf-438d-e0be-2a69a2b6623c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n    (layers): ModuleList(\n      (0-27): 28 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n          (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n          (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loading & Basic Preprocessing"
      ],
      "metadata": {
        "id": "XOU9NoAdG2QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, load_dataset\n",
        "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
        "dataset"
      ],
      "metadata": {
        "trusted": true,
        "id": "0EKO5ctyG2QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = dataset[\"train\"].select(range(500))\n",
        "dataset_validation = dataset[\"validation\"].select(range(100))\n",
        "dataset_test = dataset[\"test\"].select(range(100))\n",
        "\n",
        "print(dataset_train)\n",
        "print(dataset_validation)\n",
        "print(dataset_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "rr3kHwXjG2QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dataset to a pandas DataFrame\n",
        "df = dataset_train.to_pandas()\n",
        "\n",
        "# View the first 5 rows of the DataFrame\n",
        "first_five_rows = df.head()\n",
        "print(first_five_rows)"
      ],
      "metadata": {
        "trusted": true,
        "id": "26t-unXiG2QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = dataset_test.to_pandas()"
      ],
      "metadata": {
        "trusted": true,
        "id": "95Ly8HfEG2QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero Shot Prompt Engineering"
      ],
      "metadata": {
        "id": "2kjZL9-dG2QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"Summarize the following article.\"\"\"\n",
        "\n",
        "\n",
        "syntheses_with_gemma = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    article = row['article']\n",
        "\n",
        "    prompt = f\"\"\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n{article}\\n<end_of_turn>\\n\"\"\"\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate response\n",
        "    output = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=100)\n",
        "\n",
        "    # Decode the response\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    #print(generated_text)\n",
        "\n",
        "    #Finding Extracted Answer\n",
        "    index_of_answer = generated_text.find(\"<end_of_turn>\")\n",
        "\n",
        "    # Extract the text after \"Answer:\"\n",
        "    model_summary = generated_text[index_of_answer + len(\"<end_of_turn>\"):].strip()\n",
        "    syntheses_with_gemma.append(model_summary)\n",
        "    #print(answer_text)\n",
        "    print(idx+1)\n",
        "\n",
        "    #Comparing the answer with the base answer\n",
        "    dash_line = '-'.join('' for x in range(100))\n",
        "    summary = df.loc[idx, 'highlights']\n",
        "#     print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "#     print(dash_line)\n",
        "#     print(f'MODEL GENERATION - ZERO SHOT:\\n{model_summary}')\n",
        "#     print(dash_line)\n",
        "\n",
        "#Appending it to main file\n",
        "df['Generated_BY_GEMMA'] = syntheses_with_gemma\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "I7I5Bn7tG2QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7gA_3K79G2QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "Zero_Shot_Gemma = df.rename(columns={'article': 'article', 'highlights': 'highlights', 'id': 'id', 'Generated_BY_Gemma': 'Generated_BY_Gemma'})\n",
        "Zero_Shot_Gemma.to_csv('Zero_Shot_Gemma.csv', index=False)\n",
        "FileLink('Zero_Shot_Gemma.csv')"
      ],
      "metadata": {
        "trusted": true,
        "id": "oPsf7puZG2QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero Shot Test"
      ],
      "metadata": {
        "id": "m7TSdoPEG2QZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLUE, ROUGE 1, ROUGE 2 and ROUGE L score"
      ],
      "metadata": {
        "id": "KNNsHojSG2QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "!pip install scikit-learn\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "X8akArVGG2Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(reference, candidate):\n",
        "    # Tokenize reference and candidate\n",
        "    reference_tokens = nltk.word_tokenize(reference)\n",
        "    candidate_tokens = nltk.word_tokenize(candidate)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=SmoothingFunction().method7)\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, candidate)\n",
        "\n",
        "    return bleu_score, scores"
      ],
      "metadata": {
        "trusted": true,
        "id": "18l4VSIaG2Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through rows of the DataFrame and calculate scores\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "bleu_scores = []\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rougeL_scores = []\n",
        "\n",
        "for index, row in Zero_Shot_Gemma.iterrows():\n",
        "    reference = row['highlights']\n",
        "    candidate = row['Generated_BY_GEMMA']\n",
        "\n",
        "    bleu_score, rouge_scores = calculate_metrics(reference, candidate)\n",
        "\n",
        "    bleu_scores.append(bleu_score)\n",
        "    rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
        "    rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
        "    rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
        "\n",
        "# Calculate average scores\n",
        "avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "avg_rouge1_score = sum(rouge1_scores) / len(rouge1_scores)\n",
        "avg_rouge2_score = sum(rouge2_scores) / len(rouge2_scores)\n",
        "avg_rougeL_score = sum(rougeL_scores) / len(rougeL_scores)\n",
        "\n",
        "# Print average scores\n",
        "print(f\"Average BLEU Score: {avg_bleu_score}\")\n",
        "print(f\"Average ROUGE-1 Score: {avg_rouge1_score}\")\n",
        "print(f\"Average ROUGE-2 Score: {avg_rouge2_score}\")\n",
        "print(f\"Average ROUGE-L Score: {avg_rougeL_score}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "dFMhJV7XG2Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score"
      ],
      "metadata": {
        "trusted": true,
        "id": "-stSBev1G2Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize lists to store scores\n",
        "avg_precision_list = []\n",
        "avg_recall_list = []\n",
        "avg_f1_list = []\n",
        "\n",
        "# Iterate through rows of the DataFrame and calculate scores\n",
        "for index, row in Zero_Shot_Gemma.iterrows():\n",
        "    reference = [row['highlights']]\n",
        "    candidate = [row['Generated_BY_GEMMA']]\n",
        "\n",
        "    # Calculate BERTScore\n",
        "    P, R, F1 = score(candidate, reference, lang=\"en\")\n",
        "\n",
        "    avg_precision = P.mean().item()\n",
        "    avg_recall = R.mean().item()\n",
        "    avg_f1 = F1.mean().item()\n",
        "\n",
        "    avg_precision_list.append(avg_precision)\n",
        "    avg_recall_list.append(avg_recall)\n",
        "    avg_f1_list.append(avg_f1)\n",
        "\n",
        "# Calculate average precision, recall, and F1 score\n",
        "avg_precision = sum(avg_precision_list) / len(avg_precision_list)\n",
        "avg_recall = sum(avg_recall_list) / len(avg_recall_list)\n",
        "avg_f1 = sum(avg_f1_list) / len(avg_f1_list)\n",
        "\n",
        "# Print average precision, recall, and F1 score\n",
        "print(f\"Average Precision: {avg_precision}\")\n",
        "print(f\"Average Recall: {avg_recall}\")\n",
        "print(f\"Average F1 Score: {avg_f1}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9iCNDrj9G2Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune Gemma"
      ],
      "metadata": {
        "id": "4C2RyQmqG2Qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, load_dataset\n",
        "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:15.296696Z",
          "iopub.execute_input": "2024-05-04T09:45:15.296985Z",
          "iopub.status.idle": "2024-05-04T09:45:24.685672Z",
          "shell.execute_reply.started": "2024-05-04T09:45:15.296960Z",
          "shell.execute_reply": "2024-05-04T09:45:24.684600Z"
        },
        "trusted": true,
        "id": "IK7M1zRIG2Qa",
        "outputId": "011716a8-e829-46f4-a249-48fada06fc04"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 11490\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = dataset[\"train\"].select(range(500))\n",
        "dataset_validation = dataset[\"validation\"].select(range(100))\n",
        "dataset_test = dataset[\"test\"].select(range(100))\n",
        "\n",
        "print(dataset_train)\n",
        "print(dataset_validation)\n",
        "print(dataset_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:24.687026Z",
          "iopub.execute_input": "2024-05-04T09:45:24.687871Z",
          "iopub.status.idle": "2024-05-04T09:45:24.701358Z",
          "shell.execute_reply.started": "2024-05-04T09:45:24.687834Z",
          "shell.execute_reply": "2024-05-04T09:45:24.700387Z"
        },
        "trusted": true,
        "id": "-GY2tF0QG2Qa",
        "outputId": "b4c48871-29a0-42ee-dbc4-4f88f74c1378"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Dataset({\n    features: ['article', 'highlights', 'id'],\n    num_rows: 500\n})\nDataset({\n    features: ['article', 'highlights', 'id'],\n    num_rows: 100\n})\nDataset({\n    features: ['article', 'highlights', 'id'],\n    num_rows: 100\n})\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = dataset_train.to_pandas()\n",
        "dataset_validation = dataset_validation.to_pandas()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:24.702678Z",
          "iopub.execute_input": "2024-05-04T09:45:24.703723Z",
          "iopub.status.idle": "2024-05-04T09:45:24.761321Z",
          "shell.execute_reply.started": "2024-05-04T09:45:24.703684Z",
          "shell.execute_reply": "2024-05-04T09:45:24.760580Z"
        },
        "trusted": true,
        "id": "DmxV-rPoG2Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "prompts = []\n",
        "for idx, row in dataset_train.iterrows():\n",
        "    article = row['article']\n",
        "    summary = row['highlights']\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"Summarize the following article.\"\"\"\n",
        "\n",
        "    prompt = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n{article}\\n<end_of_turn>\\n<start_of_turn>model\\n{summary}\\n<end_of_turn>\"\n",
        "    prompts.append(prompt)\n",
        "\n",
        "dataset = Dataset.from_pandas(pd.DataFrame({'text': prompts}))\n",
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:24.762271Z",
          "iopub.execute_input": "2024-05-04T09:45:24.762514Z",
          "iopub.status.idle": "2024-05-04T09:45:24.820392Z",
          "shell.execute_reply.started": "2024-05-04T09:45:24.762492Z",
          "shell.execute_reply": "2024-05-04T09:45:24.819392Z"
        },
        "trusted": true,
        "id": "J2PM4fx_G2Qb",
        "outputId": "a7eeda89-2d7d-4755-a44d-6262136cbbb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['text'],\n    num_rows: 500\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = []\n",
        "for idx, row in dataset_validation.iterrows():\n",
        "    article = row['article']\n",
        "    summary = row['highlights']\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"Summarize the following article.\"\"\"\n",
        "\n",
        "    prompt = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n{article}\\n<end_of_turn>\\n<start_of_turn>model\\n{summary}\\n<end_of_turn>\"\n",
        "    prompts.append(prompt)\n",
        "\n",
        "eval_dataset = Dataset.from_pandas(pd.DataFrame({'text': prompts}))\n",
        "eval_dataset\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:24.821582Z",
          "iopub.execute_input": "2024-05-04T09:45:24.821891Z",
          "iopub.status.idle": "2024-05-04T09:45:24.842480Z",
          "shell.execute_reply.started": "2024-05-04T09:45:24.821853Z",
          "shell.execute_reply": "2024-05-04T09:45:24.841697Z"
        },
        "trusted": true,
        "id": "JV0DqhiqG2Qb",
        "outputId": "145503f1-74a7-4c8f-9b54-b038b98563dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['text'],\n    num_rows: 100\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:24.843687Z",
          "iopub.execute_input": "2024-05-04T09:45:24.843997Z",
          "iopub.status.idle": "2024-05-04T09:45:24.856976Z",
          "shell.execute_reply.started": "2024-05-04T09:45:24.843972Z",
          "shell.execute_reply": "2024-05-04T09:45:24.856092Z"
        },
        "trusted": true,
        "id": "bIOIglR2G2Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:24.857940Z",
          "iopub.execute_input": "2024-05-04T09:45:24.858200Z",
          "iopub.status.idle": "2024-05-04T09:45:24.885453Z",
          "shell.execute_reply.started": "2024-05-04T09:45:24.858176Z",
          "shell.execute_reply": "2024-05-04T09:45:24.884583Z"
        },
        "trusted": true,
        "id": "Nsose382G2Qb",
        "outputId": "d047422a-1481-415f-cb03-18a3843312dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n    (layers): ModuleList(\n      (0-27): 28 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n          (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n          (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=64,\n",
        "    # target_modules=[\"query_key_value\"],\n",
        "    target_modules=['o_proj', 'q_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj'], #specific to Gemma models.\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:24.886730Z",
          "iopub.execute_input": "2024-05-04T09:45:24.887072Z",
          "iopub.status.idle": "2024-05-04T09:45:25.739691Z",
          "shell.execute_reply.started": "2024-05-04T09:45:24.887039Z",
          "shell.execute_reply": "2024-05-04T09:45:25.738895Z"
        },
        "trusted": true,
        "id": "lb5TNpmGG2Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:25.740826Z",
          "iopub.execute_input": "2024-05-04T09:45:25.741200Z",
          "iopub.status.idle": "2024-05-04T09:45:25.766968Z",
          "shell.execute_reply.started": "2024-05-04T09:45:25.741166Z",
          "shell.execute_reply": "2024-05-04T09:45:25.766106Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "72029378606247ff892ac07fd0f58980"
          ]
        },
        "id": "sxAyXlCbG2Qb",
        "outputId": "cc45f9c0-b4a9-4188-9a62-f6a16737eb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72029378606247ff892ac07fd0f58980"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import TrainingArguments\n",
        "# from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "# # Define the model initialization kwargs\n",
        "# model_init_kwargs = {}\n",
        "\n",
        "# # Initialize the SFTConfig with model_init_kwargs\n",
        "# config = SFTConfig(model_init_kwargs=model_init_kwargs, output_dir=\"Fine_Tuned_Gemma\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "YZq90PlhG2Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "training_arguments = SFTConfig(\n",
        "    \"Fine_Tuned_Gemma\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=0.3,\n",
        "    num_train_epochs=5,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    warmup_ratio=0.05,\n",
        "    save_strategy=\"epoch\",\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=42,\n",
        "    push_to_hub = True,\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:35.602952Z",
          "iopub.execute_input": "2024-05-04T09:45:35.603360Z",
          "iopub.status.idle": "2024-05-04T09:45:35.633939Z",
          "shell.execute_reply.started": "2024-05-04T09:45:35.603325Z",
          "shell.execute_reply": "2024-05-04T09:45:35.633081Z"
        },
        "trusted": true,
        "id": "vUsrnhlmG2Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:36.352216Z",
          "iopub.execute_input": "2024-05-04T09:45:36.352593Z",
          "iopub.status.idle": "2024-05-04T09:45:36.357371Z",
          "shell.execute_reply.started": "2024-05-04T09:45:36.352560Z",
          "shell.execute_reply": "2024-05-04T09:45:36.356376Z"
        },
        "trusted": true,
        "id": "_qoWdWO6G2Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=100,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:38.971457Z",
          "iopub.execute_input": "2024-05-04T09:45:38.972200Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "976edb7622be4ec9ae0e7fc1e4e9bf49",
            "aad22bf0674943eaa64eacfdf6b48020"
          ]
        },
        "id": "z2PUzU17G2Qc",
        "outputId": "9a53dbf1-db96-4e86-c4ae-8d990760d09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "976edb7622be4ec9ae0e7fc1e4e9bf49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/100 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aad22bf0674943eaa64eacfdf6b48020"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmondol007\u001b[0m (\u001b[33mdeep-quest\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.6"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240504_094546-sn3czdnm</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/deep-quest/huggingface/runs/sn3czdnm' target=\"_blank\">stellar-destroyer-27</a></strong> to <a href='https://wandb.ai/deep-quest/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/deep-quest/huggingface' target=\"_blank\">https://wandb.ai/deep-quest/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/deep-quest/huggingface/runs/sn3czdnm' target=\"_blank\">https://wandb.ai/deep-quest/huggingface/runs/sn3czdnm</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='99' max='155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 99/155 31:47 < 18:21, 0.05 it/s, Epoch 3.14/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>31</td>\n      <td>3.091100</td>\n      <td>3.139008</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>1.958200</td>\n      <td>2.304088</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>1.772300</td>\n      <td>2.319348</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Checkpoint destination directory Fine_Tuned_Gemma/checkpoint-31 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory Fine_Tuned_Gemma/checkpoint-62 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory Fine_Tuned_Gemma/checkpoint-93 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "o7EvLewFG2Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import transformers\n",
        "\n",
        "model_id_1 = '/kaggle/working/Fine_Tuned_Gemma'\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "fine_tuned_gemma = AutoModelForCausalLM.from_pretrained(model_id_1, quantization_config=bnb_config, device_map=\"auto\")\n",
        "tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1)\n",
        "tokenizer_1.pad_token = tokenizer_1.eos_token\n",
        "tokenizer_1.padding_side = \"right\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "P3FUeuE-G2Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = dataset_test.to_pandas()"
      ],
      "metadata": {
        "trusted": true,
        "id": "y21k1gaKG2Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"Summarize the following article.\"\"\"\n",
        "\n",
        "\n",
        "syntheses_with_gemma = []\n",
        "\n",
        "for idx, row in dataset_test.iterrows():\n",
        "    article = row['article']\n",
        "\n",
        "    prompt = f\"\"\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n{article}\\n<end_of_turn>\\n\"\"\"\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer_1(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate response\n",
        "    output = fine_tuned_gemma.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=100)\n",
        "\n",
        "    # Decode the response\n",
        "    generated_text = tokenizer_1.decode(output[0], skip_special_tokens=True)\n",
        "    #print(generated_text)\n",
        "\n",
        "    #Finding Extracted Answer\n",
        "    index_of_answer = generated_text.find(\"<end_of_turn>\")\n",
        "\n",
        "    # Extract the text after \"Answer:\"\n",
        "    model_summary = generated_text[index_of_answer + len(\"<end_of_turn>\"):].strip()\n",
        "    syntheses_with_gemma.append(model_summary)\n",
        "    #print(answer_text)\n",
        "    print(idx+1)\n",
        "\n",
        "    #Comparing the answer with the base answer\n",
        "    dash_line = '-'.join('' for x in range(100))\n",
        "    summary = dataset_test.loc[idx, 'highlights']\n",
        "#     print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "#     print(dash_line)\n",
        "#     print(f'MODEL GENERATION - ZERO SHOT:\\n{model_summary}')\n",
        "#     print(dash_line)\n",
        "\n",
        "#Appending it to main file\n",
        "dataset_test['Generated_BY_GEMMA'] = syntheses_with_gemma\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "_bFT9vw7G2Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "8xDqJvr-G2Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "Fine_Tune_Gemma = dataset_test.rename(columns={'article': 'article', 'highlights': 'highlights', 'id': 'id', 'Generated_BY_Gemma': 'Generated_BY_Gemma'})\n",
        "Fine_Tune_Gemma.to_csv('Fine_Tune_Gemma.csv', index=False)\n",
        "FileLink('Fine_Tune_Gemma.csv')"
      ],
      "metadata": {
        "trusted": true,
        "id": "TQSJ2ms-G2Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "WW_R6y4KG2Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "!pip install scikit-learn\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "vbE2xB3rG2Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(reference, candidate):\n",
        "    # Tokenize reference and candidate\n",
        "    reference_tokens = nltk.word_tokenize(reference)\n",
        "    candidate_tokens = nltk.word_tokenize(candidate)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=SmoothingFunction().method7)\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, candidate)\n",
        "\n",
        "    return bleu_score, scores"
      ],
      "metadata": {
        "trusted": true,
        "id": "r0xEM9VqG2Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through rows of the DataFrame and calculate scores\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "bleu_scores = []\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rougeL_scores = []\n",
        "\n",
        "for index, row in Fine_Tune_Gemma.iterrows():\n",
        "    reference = row['highlights']\n",
        "    candidate = row['Generated_BY_GEMMA']\n",
        "\n",
        "    bleu_score, rouge_scores = calculate_metrics(reference, candidate)\n",
        "\n",
        "    bleu_scores.append(bleu_score)\n",
        "    rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
        "    rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
        "    rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
        "\n",
        "# Calculate average scores\n",
        "avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "avg_rouge1_score = sum(rouge1_scores) / len(rouge1_scores)\n",
        "avg_rouge2_score = sum(rouge2_scores) / len(rouge2_scores)\n",
        "avg_rougeL_score = sum(rougeL_scores) / len(rougeL_scores)\n",
        "\n",
        "# Print average scores\n",
        "print(f\"Average BLEU Score: {avg_bleu_score}\")\n",
        "print(f\"Average ROUGE-1 Score: {avg_rouge1_score}\")\n",
        "print(f\"Average ROUGE-2 Score: {avg_rouge2_score}\")\n",
        "print(f\"Average ROUGE-L Score: {avg_rougeL_score}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "buPm4F98G2Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score"
      ],
      "metadata": {
        "trusted": true,
        "id": "vRp8zNTpG2Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize lists to store scores\n",
        "avg_precision_list = []\n",
        "avg_recall_list = []\n",
        "avg_f1_list = []\n",
        "\n",
        "# Iterate through rows of the DataFrame and calculate scores\n",
        "for index, row in Fine_Tune_Gemma.iterrows():\n",
        "    reference = [row['highlights']]\n",
        "    candidate = [row['Generated_BY_GEMMA']]\n",
        "\n",
        "    # Calculate BERTScore\n",
        "    P, R, F1 = score(candidate, reference, lang=\"en\")\n",
        "\n",
        "    avg_precision = P.mean().item()\n",
        "    avg_recall = R.mean().item()\n",
        "    avg_f1 = F1.mean().item()\n",
        "\n",
        "    avg_precision_list.append(avg_precision)\n",
        "    avg_recall_list.append(avg_recall)\n",
        "    avg_f1_list.append(avg_f1)\n",
        "\n",
        "# Calculate average precision, recall, and F1 score\n",
        "avg_precision = sum(avg_precision_list) / len(avg_precision_list)\n",
        "avg_recall = sum(avg_recall_list) / len(avg_recall_list)\n",
        "avg_f1 = sum(avg_f1_list) / len(avg_f1_list)\n",
        "\n",
        "# Print average precision, recall, and F1 score\n",
        "print(f\"Average Precision: {avg_precision}\")\n",
        "print(f\"Average Recall: {avg_recall}\")\n",
        "print(f\"Average F1 Score: {avg_f1}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "F821pubTG2Qd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}